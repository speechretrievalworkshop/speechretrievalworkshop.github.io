<html>
  <head>
    <title>Workshop on Audio Collection Human Interaction(AudioCHI 2022)</title>
    <style type = "text/css">
      @font-face { font-family: ETBembo;
             src: url("ETBembo-RomanLF.ttf"); }
              @font-face { font-family: ETBembo;
             src: url("ETBembo-DisplayItalic.ttf");
             font-weight: normal;
             font-style: italic; }

html { font-size: 15px; }

body { width: 57.5%;
       margin-left: auto;
       margin-right: auto;
       padding-left: 12.5%;
       font-family: ETBembo, Palatino, "Palatino Linotype", "Palatino LT STD", "Book Antiqua", Georgia, serif;
       background-color: #ffffff;
       color: #111;
       max-width: 1400px;
       counter-reset: sidenote-counter; }
      h1 { font-weight: 400;
     margin-top: 4rem;
     margin-bottom: 1.5rem;
     font-size: 3.2rem;
     line-height: 1; }
      h2 { font-style: italic;
     font-weight: 400;
     margin-top: 2.1rem;
     margin-bottom: 0;
     font-size: 2.2rem;
     line-height: 1; }
p.subtitle { font-style: italic;
             margin-top: 1rem;
             margin-bottom: 1rem;
             font-size: 1.8rem;
             display: block;
             line-height: 1; }
      
p, ol, ul { font-size: 1.4rem; }

p { line-height: 2rem;
    margin-top: 1.4rem;
    margin-bottom: 1.4rem;
    padding-right: 0;
    vertical-align: baseline; }

blockquote p { font-size: 1.1rem;
               width: 50%; }

blockquote footer { width: 50%;
                    text-align: right; }

ul {
     -webkit-padding-start: 5%;
     -webkit-padding-end: 5%;
     list-style-type: none; }

li { padding: 0.5rem 0; }
      
      a { color: #111;
    text-decoration: none;
    border-bottom: 1px solid #777;
    padding-bottom: 1px; }

img { max-width: 100%; }

.sidenote, .marginnote { float: right;
                         clear: right;
                         margin-right: -60%;
                         width: 50%;
                         margin-top: 0;
                         margin-bottom: 0;
                         font-size: 1.0rem;
                         line-height: 1.6;
                         vertical-align: baseline;
                         position: relative; }

    </style>
 </head>
 
  <body>
    <article>
    <h1>AudioCHI 2022</h1>
<p class="subtitle">
  Workshop on Audio Collection Human Interaction
    </p>
 <p>
Organised in conjunction with the <a href="https://ai.ur.de/chiir2022/home">ACM SIGIR Conference on
Human Information Interaction
    and Retrieval (CHIIR 2022)</a> online, on 
    Monday 14 March, 2022
    </p>
    <h2>Aims</h2>
    <p>
    The AudioCHI 2022 workshop will focus on human engagement with spoken material in search settings, 
    including live stream audio and collections. 
    This moves beyond spoken content retrieval to examine interaction with this content in the search process. 
    </p>
    <h2>Challenge questions</h2>
    <p>
      <ol>
        <li>Use cases --- what is the parameter space of use cases for search, retrieval, exploration of speech audio?</li>
        <li>Features --- What are some interesting audio features we do not pay enough attention to?</li>
        <li>Shared tasks --- Can we suggest some new shared tasks that would span the above space interestingly?</li>
    </ol>


    </p>
    <h2>Program and Schedule</h2>
    <p>
      We will kick off at 1400 EST (9am EST) and run till 1800 EST (1pm EST)
       
      The program which will be scheduled for a half day to fit European afternoon and US East Coast morning working hours, will include a keynote, 
      a paper presentation, and an open discussion session.  
      <ul>
      <li> 1400 CET / 9am EST: Intro and Overview: initial discussion of challenge questions</li>
      <li> 1500 CET / 10am EST: Presentation: Moreno La Quatra <i>"<a href="Bimodal.pdf">Bi-modal Architectures for Deeper User Preference Understanding from Spoken Content"</a></i></li>
      <li> 1530 CET / 10:30am EST: coffee break</li>
      <li> 1600 CET / 11am EST: Keynote:  <A HREF="https://www.umiacs.umd.edu/people/oard">Doug Oard</a> <i><a href="Talking with the Planet.pdf">"Talking with the Planet"</a></i></li>
      <li> 1700 CET / noon EST: Discussion and write up of challenge questions</li>
      <li> 1800 CET / 1pm EST: close</li>
    </ul>
</p>
    
    
    <h2>Call for participation</h2>

  <!--
   <p>
     We invite interested participants to submit a short position paper of about one to two pages introducing their interests in the topics of the workshop to the e-mail address below. 
</p>
   <p>
     We also invite all workshop participants to present a <i>speech feature</i> or <i>usage scenario</i> in access to human spoken data and its opportunities or challenges for discussion in the AudioCHI 2022 "Feature Festival" using a rapid one-slide presentation. An audio clip illustrating your proposal would be appreciated (but not required). 
</p> -->
        <h2>Interacting with Spoken Content Collections</h2>

      <p>
Spoken material comes in many forms, including for example: factual or entertaining (or both!), 
      current  or historical interests, local or global, single speaker or multi-person dialogues. 
      Users engage with spoken material for a variety of reasons, including entertainment, current affairs, education, and research. 
    </p>
    <p>
While there has been considerable previous work studying spoken document retrieval or more generally spoken content retrieval, 
      AudioCHI 2022 will be the first meeting to explore user engagement with audio content, 
      including the use of extracted verbal and non-verbal features for creation of rich content representations. 
      AudioCHI will explore human factors in interaction with spoken audio content, associated existing work 
      in the field of spoken content retrieval. 
      The workshop seeks to bring together researchers in spoken content retrieval with 
      researchers in interactive information retrieval and researchers interested in engagement with speech data, 
      to examine opportunities and challenges for advancing the technologies for speech search and interaction with spoken content.
    </p>
 
<!--
    
<h2>Important Dates</h2>
    <ul>
      <li>    
        Submission of position papers: <br>Friday, February 11, 2022
      </li>
      <li>    
        Notification of acceptance: <br>Friday, February 25, 2022
      </li>
      <li>
      Workshop day: <br>Monday 14 March, 2022
      </li>
    </ul>

-->
      <h2>Organisers</h2>
    <ul>
      <li>
    Gareth J. F. Jones, Dublin City University
      </li><li>
Maria Eskevich, CLARIN ERIC      
      </li><li>

    Ben Carterette, Joana Correia, Rosie Jones, Jussi Karlgren, Spotify      
      </li><li>
 Ian Soboroff, National Institute of Standards and Technology, United States
      </li>
    </ul>
    <h2>Contact</h2>
    <p>
      audio-chi@googlegroups.com
    </p>
  </article>
  </body>
</html>
