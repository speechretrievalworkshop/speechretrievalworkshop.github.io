<html>
  <head>
    <title>Workshop on Audio Collection Human Interaction(AudioCHI 2022)</title>
    <style type = "text/css">
      @font-face { font-family: ETBembo;
             src: url("ETBembo-RomanLF.ttf"); }
@font-face { font-family: ETBembo;
             src: url("ETBembo-DisplayItalic.ttf");
             font-weight: normal;
             font-style: italic; }

html { font-size: 15px; }

body { width: 87.5%;
       margin-left: auto;
       margin-right: auto;
       padding-left: 12.5%;
       font-family: ETBembo, Palatino, "Palatino Linotype", "Palatino LT STD", "Book Antiqua", Georgia, serif;
       background-color: #fffff8;
       color: #111;
       max-width: 1400px;
       counter-reset: sidenote-counter; }
      h1 { font-weight: 400;
     margin-top: 4rem;
     margin-bottom: 1.5rem;
     font-size: 3.2rem;
     line-height: 1; }
      h2 { font-style: italic;
     font-weight: 400;
     margin-top: 2.1rem;
     margin-bottom: 0;
     font-size: 2.2rem;
     line-height: 1; }
p.subtitle { font-style: italic;
             margin-top: 1rem;
             margin-bottom: 1rem;
             font-size: 1.8rem;
             display: block;
             line-height: 1; }
      
      p, ol, ul { font-size: 1.4rem; }

p { line-height: 2rem;
    margin-top: 1.4rem;
    margin-bottom: 1.4rem;
    padding-right: 0;
    vertical-align: baseline; }

blockquote p { font-size: 1.1rem;
               width: 50%; }

blockquote footer { width: 50%;
                    text-align: right; }

ul { width: 45%;
     -webkit-padding-start: 5%;
     -webkit-padding-end: 5%;
     list-style-type: none; }

li { padding: 0.5rem 0; }
      
      a { color: #111;
    text-decoration: none;
    border-bottom: 1px solid #777;
    padding-bottom: 1px; }

img { max-width: 100%; }

.sidenote, .marginnote { float: right;
                         clear: right;
                         margin-right: -60%;
                         width: 50%;
                         margin-top: 0;
                         margin-bottom: 0;
                         font-size: 1.0rem;
                         line-height: 1.6;
                         vertical-align: baseline;
                         position: relative; }

    </style>
 </head>
 
  <body>
    <h1>AudioCHI 2022</h1>
<p class="subtitle">
  Workshop on Audio Collection Human Interaction
    </p>
 <p>
Organised in conjunction with the <a href="https://ai.ur.de/chiir2022/home">ACM SIGIR Conference on
Human Information Interaction
    and Retrieval (CHIIR 2022)</a> in Regensburg, Bavaria on 
    Monday 14 March, 2022
    </p>
<h2>Aims</h2>
    
    <p> The AudioCHI 2022 workshop will focus on human engagement with spoken material in search settings, including live stream audio and collections. </p>
    <p>
      Spoken material comes in many forms, including for example: factual or entertaining (or both!), timely or of historical interest, local or global, single speaker or conversations. 
      Users engage with spoken material for a variety of reasons, including entertainment, current affairs, education, and research. 
    While there has been considerable previous work studying spoken document retrieval or more generally spoken content retrieval, 
      AudioCHI 2022 will be the first meeting to 
    explore user engagement with audio content, including the use of 
    content analysis
    to provide verbal and non-verbal features to provide 
    rich content representations and human factors in interaction with spoken audio content, and their interaction with more established topics relating to spoken content retrieval.
    The workshop seeks to bring together researchers in spoken content retrieval with the expertise of the CHIIR community in interactive information retrieval to examine opportunities and challenges for advancing the technologies for search and interaction with spoken content.  
    </p>
    
<h2>Call for participation</h2>
    <p>
      We invite interested participants to submit a short position paper of about one to two pages to the e-mail address below
      for presentation at the workshop in boaster + poster form. 
      We also invite workshop participants to present a <i>feature</i> of interest for analysis of human spoken data or a 
      <i>use case</i> to further discussion of information access to human speech in a "Feature Festival" with 
      rapid one-slide presentations. <p>
      An audio clip to accompany the presentation would be appreciated (but not required)! </p>
    </p>
  <h2>Program</h2>
    <p>
      The program will include invited keynote speakers, a Rapid Fire Feature Festival, a poster session, and breakout groups for discussion. 
    </p>
<h2>Important Dates</h2>
    <ul>
      <li>    
        Submission of position papers: Friday, February 11, 2022
      </li>
      <li>    
        Notification of acceptance: Friday, February 25, 2022
      </li>
      <li>
      Workshop day: Monday 14 March, 2022
      </li>
    </ul>
      <h2>Organisers</h2>
    <ul>
      <li>
    Gareth J. F. Jones, Dublin City University
      </li><li>
Maria Eskevich, CLARIN ERIC      
      </li><li>

    Ben Carterette, Joana Correia, Rosie Jones, Jussi Karlgren, Spotify      
      </li><li>
 Ian Soboroff, National Institute of Standards and Technology, United States
      </li>
    </ul>
    <h2>Contact</h2>
    <p>
      audio-chi@googlegroups.com
    </p>
  </body>
</html>
